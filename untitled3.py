# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VSCxcWESmftO1Nta-1YcplV_kOFIFUC5
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from collections import Counter
import re

# 1. Load the Dataset
data = pd.read_csv('/Restaurant_Reviews.tsv', sep='\t')  # Replace with your dataset path
print(data.head())  # Display the first few rows of the dataset

def clean_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    return text

data['Review'] = data['Review'].apply(clean_text)

def clean_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    return text

# Apply the cleaning function to the 'Review' column
data['Cleaned_Review'] = data['Review'].apply(clean_text)

# Display original and cleaned reviews side by side
print(data[['Review', 'Cleaned_Review']].head(5))

data['tokens'] = data['Review'].apply(lambda x: x.split())
print(data[['Review', 'tokens']].head(5))

for index, row in data.iterrows():
    print(f"Review: {row['Review']}")
    print(f"Tokens: {row['tokens']}")
    print()  # Print a newline for better readability

def create_training_data(sentences, window_size=2):
    training_data = []
    for sentence in sentences:
        for i, word in enumerate(sentence):
            start = max(0, i - window_size)
            end = min(len(sentence), i + window_size + 1)
            context_words = [sentence[j] for j in range(start, end) if j != i]
            training_data.append((context_words, word))
    return training_data

training_data = create_training_data(data['tokens'].tolist())

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, Flatten

# Prepare vocabulary and mapping
all_words = [word for context, target in training_data for word in context] + [target for context, target in training_data]
word_counts = Counter(all_words)
vocab_size = len(word_counts)
word_to_index = {word: i for i, (word, _) in enumerate(word_counts.items())}

# Create input-output pairs for CBOW model training
X = []
y = []

for context_words, target_word in training_data:
    X.append([word_to_index[word] for word in context_words])
    y.append(word_to_index[target_word])

X = tf.keras.preprocessing.sequence.pad_sequences(X)  # Pad sequences to make them of equal length
y = np.array(y)

# Build CBOW model
embedding_dim = 50  # Size of embeddings

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X.shape[1]))
model.add(Flatten())
model.add(Dense(vocab_size, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X, y, epochs=5)

# Generate Word Embeddings
embeddings = model.layers[0].get_weights()[0]

def classify_review(review):
    cleaned_review = clean_text(review)
    tokens = cleaned_review.split()

    # Get embeddings for each token and calculate average embedding
    valid_tokens = [word for word in tokens if word in word_to_index]
    if not valid_tokens:
        return "Unknown"

    avg_embedding = np.mean(embeddings[[word_to_index[word] for word in valid_tokens]], axis=0)

    # Predict sentiment based on average embedding magnitude (this is a simplified approach)
    if np.linalg.norm(avg_embedding) > 0.5:
        return "Positive"
    else:
        return "Negative"

# Classify sentiments for all reviews in the dataset
data['Predicted_Sentiment'] = data['Review'].apply(classify_review)

# Display original reviews, cleaned reviews, and predicted sentiments
print(data[['Review', 'Cleaned_Review', 'Predicted_Sentiment']].head(20))  # Display first 20 rows

# Document Findings and Insights (you can print or save this summary)
report_summary = {
    'Total Reviews': len(data),
}

print("Report Summary:")
for key, value in report_summary.items():
    print(f"{key}: {value}")